---
title: "Wasted: The wine dataset"
author: 'Jose Francisco Endrinal'
output: github_document
---

# Wasted on data: Exploring the wine dataset

Whether you're a drinker or not, chances are that alcohol has played some part in the culture of where you live. It would be awesome to take measurements on any wine and be able to determine what kind of wine it was. In this dataset, we're sticking with three unidentified types of wine.  

The wine dataset is a series of measurements of different samples of three types of wine. In this dataset, there are thirteen characteristics that were measured:
 	1. Alcohol
 	2. Malic acid
 	3. Ash
	4. Alcalinity of ash  
 	5. Magnesium
	6. Total phenols
 	7. Flavanoids
 	8. Nonflavanoid phenols
 	9. Proanthocyanins
	10. Color intensity
 	11. Hue
 	12. OD280/OD315 of diluted wines
 	13. Proline  

Unfortunately, we do not have the units of these measurements and simply have to rely on their relative values.  

```{r warning=FALSE, message=FALSE}
# Loading package
suppressPackageStartupMessages({
  library(nnet)
  library(tidyverse)
  library(modelr)})
#Loading dataset
read_csv("data/wine.csv") %>% 
  rename(class = X1, 
         alcohol = X2, 
         mall.Ac = X3, 
         ash = X4, 
         alk.Ash = X5, 
         magnes = X6, 
         tot.Phen = X7, 
         flavan = X8, 
         nonFl.ph = X9, 
         proanth = X10, 
         col.Int = X11, 
         hue = X12, 
         dil = X13, 
         proline = X14) -> wine
```

```{r echo=FALSE}
wine
```
Looking at the dataset alone, we can see that most of these variables are continuous. But we have too many characteristics to work with, hurting the insight we could gain. Let's try to do what we did last time with our [breast cancer dataset](../breastcancer/breastcancer_post1.md). We need to see if there may be an apparent way to separate the classes and the characteristics based on visualization alone. 

### Exploratory data analysis

Let's see which variables we can use to divide the data classes. Unlike our previous post on the breast cancer dataset, this data is continuous data. Which means that we need to look at the smoothened distribution rather than the discrete occurences. 

Our visualization looks like the following: 
```{r warning=FALSE}
mutate(wine, class = as.character(class)) %>%
  mutate_if(is.numeric, scale) %>% 
  gather(key, value, -class) %>% 
  ggplot(aes(x = value)) + 
  geom_density(aes(fill = class), position = "stack") + 
  facet_wrap(~ key, ncol = 4)
```

dil, hue and tot.Phen here look like they can divide the red and blue class well. col.Int has the green and blue class on either side. proline differentiates the red and green class well. I'll try and think of a deliberate way to pick variables in this post, by this is the best I have so far. In the future, I'll be using feature selection methods that don't require expert judgement (filter, wrapper methods, etc.) and instead rely on statistical methods to weed out variables. 

Using these variables, we'd like to know if a k-means model would work in separating these three classes well and how well we can improve it. 
### Training and Testing Sets

Like in our previous post, let us make 100 training and testing sets for our dataset. 
```{r}
set.seed(2387)
tt.sets <- select(wine, 
                  class, 
                  dil, 
                  hue, 
                  tot.Phen, 
                  col.Int, 
                  proline) %>% 
  mutate(class = as.character(class) %>% as_factor) %>% 
  crossv_mc(n = 100, test = 0.3) %>% 
  mutate(ind.tr = map(train, as.integer), 
         ind.te = map(test, as.integer), 
         train.s = map(train, as_data_frame), 
         test.s = map(test, as_data_frame))
```

```{r}
tt.sets
```


### Building the multinomial regression model

```{r}
mnom <- function(df){
  multinom(class ~ 
             dil + 
             hue + 
             tot.Phen + 
             col.Int + 
             proline, 
           data = df, 
           trace = FALSE)}
```

```{r}
mnom_predict <- function(model, df){
  preds <- predict(model, newdata = select(df, -class), type = "class")}
mnom_response <- function(df){
  df[["class"]]}
```

## Training and testing the model

```{r}
# average score
mnom_mscore <- function(pred, resp){
  df <- pred == resp
  mean(df)}
# create new variables
tt.sets <- tt.sets %>% 
  mutate(model = map(train.s, mnom), 
         pred = map2(model, test.s, mnom_predict), 
         resp = map(test.s, mnom_response), 
         hit = map2_dbl(pred, resp, mnom_mscore))
score <- summarise(tt.sets, score = mean(hit))
score[[1]]
```

## Distribution of scores

```{r}
select(tt.sets, hit) %>% 
  ggplot(aes(x = hit)) + 
  geom_density(fill = "#87CEFA")
```


```{r echo=FALSE}
rm(mnom, mnom_mscore, mnom_predict, mnom_response)
rm(score, tt.sets, wine)
```

